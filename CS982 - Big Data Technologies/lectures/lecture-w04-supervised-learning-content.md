---
title: "Week 04 ‚Äî Supervised Learning (Lecture Content)"
module: "CS982"
tags: [module/CS982, type/lectures, week-04, content]
date: 2025-10-15
lecturer: "Joseph El Gemayel, Ph.D., FHEA"
---

# Supervised Learning
*Week 04 Lecture*

CS989 Big Data Fundamentals  
CS982 Big Data Technologies  
Joseph El Gemayel, Ph.D., FHEA  
Teaching Fellow  
Computer & Information Sciences  
Email: joseph.el-gemayel@strath.ac.uk  
Office: LT1214 \- Livingstone TowerLecture overview  
‚û¢ Some questions about the coursework  
‚û¢ Choosing and Evaluating Models  
‚ùñ What techniques are appropriate for what kind of problems  
‚ùñ How to evaluate the model produced  
‚û¢ Regression  
‚û¢ Classification  
‚û¢ DemoSelf-Attendance via MyPlace  
‚û¢ Only works if you are already enrolled in the module on PegasusSome questions about the coursework  
‚û¢ Can we use chatGPT or any other GenAI tool?  
‚ùñ As part of your learning process ‚Äì YES  
‚ùñ Troubleshoot errors in your code ‚Äì YES  
‚ùñ Proofreading your text ‚Äì YES  
‚ùñ Generating (part of) content or code for your coursework ‚Äì STRICTLY NO  
‚ùñ More details can be found in the ‚ÄúFrequently Asked Questions‚Äù document available on  
MyPlaceSome questions about the coursework  
‚û¢ Which dataset should I choose?  
‚ùñ Identify the topic that you are interested in before choosing the dataset  
‚ùñ Take your time in choosing the topic and the dataset  
‚ùñ It‚Äôs recommended to work with a tabular dataset  
‚ùñ Avoid time series datasets, such as predicting Bitcoin value  
‚ùñ Avoid datasets with unknown / unclear features  
‚ùñ Avoid datasets with very low number of relevant features  
‚ùñ Highly recommended: do some exploratory analysis before decidingSome questions about the coursework  
‚û¢ The dataset is too small or too large, what should I do?  
‚ùñ If too small, look for another dataset on a similar topic to merge with  
‚ùñ If too large, work on part of the dataset  
‚ùñ Whatever you do, you should mention it in the report  
‚ùñ In fact, all decisions should be well mentioned and justified in the reportSome questions about the coursework  
‚û¢ Can we use chatGPT or any other GenAI tool?  
‚û¢ Which dataset should I choose?  
‚û¢ The dataset is too small or too large, what should I do?  
‚û¢ When I should start working on the coursework? Last week ‚ò∫  
‚û¢ Don‚Äôt forget to select your group for the first part of the coursework\!\!  
‚û¢ Any other question? Post it on MyPlace or bring it to the lab sessionChoosing Methods  
‚û¢ Ultimate goal is to solve some sort of problem  
‚ùñ Increase sales or customer satisfaction  
‚ùñ Identify fraudulent transactions  
‚ùñ Portfolio management  
‚û¢ Descriptive statistics and Graphs can help understand the data  
‚û¢ Lots of statistical modelling methods, with strengths and weaknesses  
‚ùñ Need to be able to choose most appropriate method(s)  
‚û¢ Need a measure of quality of your model to ensure it performs well in productionMapping Problems to Machine Learning  
‚û¢ Supervised  
‚ùñ Data is labelled: and  
‚ùñ Algorithms learn to predict the output from input dataMapping Problems to Machine Learning  
‚û¢ Unsupervised  
‚ùñ Data is unlabelled: ???  
‚ùñ Algorithms learn to inherent structure from input dataMapping Problems to Machine Learning  
‚û¢ Unsupervised  
‚ùñ Data is unlabelled: ???  
‚ùñ Algorithms learn to inherent structure from input dataMapping Problems to Machine Learning  
‚û¢ Semi-supervised  
‚ùñ Some data is labelled, but most is unlabelled  
‚ùñ Mixture of supervised and unsupervised techniques can be usedMapping Problems to Machine Learning  
Source: Practical Data Science with R, by Nina Zumel and John MountMapping Problems to Machine Learning  
Source: Practical Data Science with R, by Nina Zumel and John MountEvaluation and Validation  
‚û¢ Evaluation  
‚ùñ Quantifying performance of the model  
‚ùñ Needs to be appropriate to goal and modelling task e.g. classification vs scoring tasks  
‚û¢ Validation  
‚ùñ Assurance that it works as well in practise as in training  
‚ùñ Common problem is lack of appropriate training data  
‚ùñ Validation techniques attempt to quantify riskEvaluation and Validation  
‚û¢ Need to split data in training set and test setEvaluation and Validation  
‚û¢ Need to split data in training set and test set  
‚û¢ Measures vary according to type of model  
‚ùñ Classification  
‚ùñ Scoring  
‚ùñ Probability Estimation  
‚ùñ Ranking  
‚ùñ ClusteringEvaluation and Validation  
‚û¢ Important to compare measures against a baseline  
‚ùñ Best model of a very simple form that you are trying to outperform  
‚ùñ Typically a constant or a model that is independent of relationships between inputs and  
outputs e.g.  
‚ùñ Represents a lower bound on desired performance  
‚û¢ Also notion of Bayes rate error  
‚ùñ Best possible model  
‚ùñ Upper bound to aspire toEvaluating Classification ModelsEvaluating Classification Models  
‚û¢ Confusion Matrix ‚Äì Summaries a  
classifiers predictions against  
known categories  
‚û¢ Basically shows what has been  
classified correctly, what has not  
and how  
Predicted  
CM Yes No  
Yes True Positive (TP) False Negative (FN)  
Type II \- Error  
True  
No False Positive (FP)  
Type I \- Error  
True Negative (TN)Evaluating Classification Models  
This Photo by Unknown Author is licensed under CC BY-SAEvaluating Classification Models  
Predicted Values  
Apple Banana Tomato  
True Values  
Tomato Banana Apple  
9 8 7  
3 2 1  
6 5 4Evaluating Classification Models  
TP  
Predicted Values  
Apple Banana Tomato  
True Values  
Tomato Banana Apple  
9 8 7  
3 2 1  
6 5 4Evaluating Classification Models  
9 8 7  
Predicted Values  
TP FN  
Apple Banana Tomato  
True Values  
Tomato Banana Apple  
3 2 1  
6 5 4Evaluating Classification Models  
9 8 7  
Predicted Values  
TP FN  
Apple Banana Tomato  
True Values  
Tomato Banana Apple  
3 2 1  
6 5 4  
FPEvaluating Classification Models  
9 8 7  
Predicted Values  
TP FN  
Apple Banana Tomato  
Tomato Banana Apple  
6 5 4  
FP TN  
True Values  
3 2 1Evaluating Classification Models  
TP  
Predicted Values  
Apple Banana Tomato  
True Values  
Tomato Banana Apple  
9 8 7  
3 2 1  
6 5 4Evaluating Classification Models  
Predicted Values  
Apple Banana Tomato  
FN  
True Values  
Tomato Banana Apple  
9 8 7  
3 2 1  
6 5 4Evaluating Classification Models  
Predicted Values  
Apple Banana Tomato  
True Values  
Tomato Banana Apple  
9 8 7  
3 2 1  
6 5 4  
FPEvaluating Classification Models  
Predicted Values  
Apple Banana Tomato  
9 8 7  
True Values  
3 2 1  
Tomato Banana Apple  
6 5 4  
FP TNEvaluating Classification Models  
‚û¢ Several measures based on confusion matrix  
‚ùñ Precision ‚Äì Fraction of items classified identified as being in a class that actually are  
TP / (TP \+ FP)  
‚ùñ Recall ‚Äì Fraction of items that are in the class that classifier detects  
TP / (TP \+ FN)  
‚ùñ F1 score ‚Äì Combines precision and recall  
2 \* (precision \* recall) / (precision \+ recall)  
‚ùñ Accuracy ‚Äì Fraction of items classified correctly  
(TP \+ TN) / (TP \+ FP \+ TN \+ FN)Evaluating Scoring Models  
‚û¢ Here we look at the residuals  
‚û¢ Essentially is the difference between  
the predicted and actual / true  
outcomesEvaluating Scoring Models  
‚û¢ Absolute Error  
(Œîx) \= |ùë•ùëñ ‚àí ùë•|  
‚û¢ Mean Absolute ErrorEvaluating Scoring Models  
‚û¢ Root Mean Square Error (RMSE) ‚Äì Square root of average square of the  
difference between predicted and actual values (i.e. how far off is prediction)  
‚û¢ R-Squared ‚Äì a statistical measure that represents the proportion of the  
variance for a dependent variable that's explained by an independent  
variable in a regression modelValidating Models  
‚û¢ Having evaluated a model on training data how well will it perform on real  
data?  
‚û¢ Common problems  
‚ùñ Bias ‚Äì Systematic error in model e.g. always under predictingValidating Models  
‚û¢ Having evaluated a model on training data how well will it perform on real  
data?  
‚û¢ Common problems  
‚ùñ Variance ‚Äì Undesirable (and non-systematic) distance between predictions and actual  
values (may be due to oversensitivity of model training  
procedure to small variations in data for example)Bias Variance Trade-OffBias Variance Trade-OffValidating Models  
‚û¢ Having evaluated a model on training data how well will it perform on real  
data?  
‚û¢ Common problems  
‚ùñ Overfitting ‚Äì Features that arise due to unrepresentative training data  
‚ùñ Non-significance ‚Äì Model that appears to show an important relation which may not hold  
in the general populationOverfittingOverfittingEnsuring Model Quality ‚Äì Held Out Data  
‚û¢ Models should not be evaluated on  
training data alone  
‚û¢ Split data in test and training set  
‚û¢ It only gives one a single point of  
estimate thoughEnsuring Model Quality ‚Äì K-Fold Cross  
Validation  
‚û¢ Models should not be evaluated on  
training data alone  
‚û¢ Repeat model construction and  
evaluation on different subsets of  
data  
‚û¢ Unbiased estimateSupervised Learning ‚Äì Models  
This Photo by Unknown Author is licensed under CC BYLinear and Logistic Regression  
‚û¢ Both methods learn a model that is a continuous function of its inputs  
‚û¢ Useful when you don‚Äôt just want to predict an outcome, but you also want to  
know the relationship between the input variables and the outcome  
‚û¢ Main differences  
‚ùñ Linear regression used to predict quantities  
‚ùñ Logistic regression used to predict probabilities or categories  
‚û¢ Good techniques to try before exploring more complex approachesLinear Regression  
‚û¢ Standard approach for trying to predict a numerical quantity (cost, volume,  
quantity etc.)  
‚ùñ If it works well \- great\!  
‚ùñ If it fails \- detailed diagnostics produced give you a good clue as to what methods you  
should try next  
‚û¢ Models the expected value of a numeric quantity (the dependent or  
response variable) in terms of numeric and categorical inputs (the  
independent or explanatory variables)Linear Regression \- Formally  
‚û¢ Suppose that  
‚ùñ y\[i\] is the numeric quantity that we want to predict  
‚ùñ And x\[i,\] is a row of inputs that corresponds to output y\[i\]  
‚û¢ Linear regression finds a fit function f(x) such that  
‚ùñ y\[i\] \~ f(x\[i,\]) \= b\[1\] x\[i,1\] \+ ... b\[n\] x\[i,n\]  
‚û¢ We want numbers b\[1\],...,b\[n\] (called the coefficients or betas) such that  
f(x\[i,\]) is as near as possible to y\[i\] for all (x\[i,\],y\[i\]) pairs in our training dataLinear Regression \- Example  
‚û¢ Try to predict a student's grade for a class from their lecture and lab  
attendance  
‚ùñ For each individual, i, assume that grade is a linear combination of lecture and lab  
attendance. i.e.  
‚ùñ final.grade\[i\] \= b.lectures \* lecture.attendance\[i\] \+ b.labs \* lab.attendance\[i\]  
‚û¢ Goal is to find the values of b.lectures and b.attendance so that the linear  
combination of lecture.attendance\[i\] and lab.attendance\[i\] comes very close  
to final.grade\[i\] for all students i in the training dataLinear Regression ‚Äì Calculating  
Coefficients  
‚û¢ Works by trying to minimise the  
residuals, the difference between  
predicted and actual outcomesLogistic Regression  
‚û¢ Member of a class of models called generalised linear models  
‚û¢ Logistic regression can directly predict values that are restricted to the (0,1)  
interval, such as probabilities  
‚û¢ Good first choice for binary classification problems  
‚û¢ Logistic regression predicts the probability y that an instance belongs to a  
specific category  
‚ùñ e.g. the probability that a flight will be delayedLogistic Regression  
‚û¢ Similar form to linear regression but  
aims to predict the probability of a set  
of outcomes mapping to a class. i.e.  
‚û¢ P\[y\[i\] in class\] \~ f(x\[i,\]) \= s(a+b\[1\]  
x\[i,1\] \+ ... b\[n\] x\[i,n\])  
‚û¢ Where, s(z) is the sigmoid function,  
defined as s(z) \= 1/(1+exp(z))Regression ‚Äì Summary  
‚û¢ Looked at two functional techniques: linear regression and logistic  
regression  
‚ùñ Functional models allow you to better explore how changes in inputs affect predictions  
‚ùñ Linear regression is a good first technique for modelling quantities  
‚ùñ Logistic regression is good first technique for modelling probabilities  
‚ùñ Models with simple forms come with very powerful summaries and diagnosticsDecision Trees  
‚û¢ Simple model type, make a prediction that is piecewise constant  
‚ùñ Split the training data into pieces and use a simple memorised constant on each piece  
‚ùñ Like machine-generated business rules  
‚û¢ Works by proposing many possible cuts in the data then choosing best cuts  
in simultaneous competing criteria of  
‚ùñ Predictive power  
‚ùñ Cross-validation strength  
‚ùñ Interaction with other chosen cutsDecision Trees  
‚û¢ Classification trees, where the target  
variable is categorical and the tree is used  
to identify the class within which a target  
variable would likely fall into  
‚û¢ Regression Trees, where the target  
variable is continuous and tree is used to  
predict its value  
‚û¢ Popular algorithms include ID3, C.45  
https://commons.wikimedia.org/wiki/File:CART\_tree\_titanic\_survivors.png  
Creative Commons Attribution-Share Alike 3.0 Unported license.Decision Trees ‚Äì iris datasetDecision Tree Regression  
Source: https://scikit-learn.org/stable/auto\_examples/tree/plot\_tree\_regression.htmlDecision Trees ‚Äì Feature importance  
Source: https://medium.com/analytics-vidhya/random-forest-on-titanic-dataset-88327a014b4dDecision Trees  
‚û¢ Can be applied to any type of data  
‚û¢ Final structure of the classifier can be quite simple  
‚û¢ Resulting trees are usually quite understandableNa√Øve Bayes  
‚û¢ Uses the notion of conditional probability  
‚û¢ Considers how each training variable is related to output  
‚û¢ Makes predictions by multiplying together effects of each variableNa√Øve Bayes  
‚û¢ Assume we have a problem instance to be classified, represented by a  
vector x=(x1, x2, ‚Ä¶. xn ), and there are a number of class outcomes, ck,  
then Na√Øve Bayes will assign a probability thus: (ùëê  
\_  
ùëò‚îÇùë•\_1,ùë•\_2, ‚Ä¶,ùë•  
\_  
ùëõ )  
‚û¢ This conditional probability can be reformulated to be more tractable  
‚û¢ Which can be written informally asNa√Øve Bayes  
Weather Play  
Sunny No  
Rain Yes  
Snow No  
Sunny Yes  
Sunny Yes  
Rain No  
Snow No  
Snow No  
Rain No  
Sunny Yes  
Weather Sunny Rain Snow All Frequency Table  
Weather No Yes  
Sunny 1 3  
Rain 2 1  
Snow 3  
Grand Total 6 4  
Likelihood Table  
No Yes  
1 3 \= 4/10 2 1 \= 3/10 3 \= 3/10 6 4  
\= 6/10 \= 4/10  
0.4  
0.3  
0.3Na√Øve Bayes  
‚û¢ Probability of playing when weather is sunnyNa√Øve Bayes  
‚û¢ Benefits  
‚ùñ Easy and fast to predict a class  
‚ùñ Performs well for categorical variables  
‚ùñ For numerical variables, a normal distribution is assumed  
‚û¢ Drawbacks  
‚ùñ If categorical variable has a category not in training set, the model will assign 0 and not  
be able to make a prediction  
‚ùñ A limitation is the assumption of independent predictionsSummary  
‚û¢ Choosing and Evaluating Models  
‚ùñ Training and test sets  
‚ùñ Measures for quantifying performance of classification and scoring algorithms  
‚û¢ Memorisation-Based Techniques  
‚ùñ Decision Trees \- Model makes prediction by choosing the summary of all training data  
that was placed in the same leaf node of the decision tree as the current example to be  
scored  
‚ùñ Na√Øve Bayes \- Prediction for a given example is the product of all the collection of  
applicable single variable models  
